# Harvester RAID1 Configuration with Optimized Storage Allocation
# OS: 200GB partition on 960GB RAID1 + 760GB /data partition for storage
# Storage: Full 1.92TB RAID1 + 760GB from OS RAID1 = ~2.68TB total

#cloud-config

hostname: hv1
fqdn: hv1.crux.truther.to

users:
  - name: rancher
    groups:
      - sudo
      - docker
    sudo: ALL=(ALL) NOPASSWD:ALL
    shell: /bin/bash
    ssh_authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCVm0gzV/a5GHRjW25h1PKomRPHxYZK1Fz7YpDi+NTwpqQvMrw6lStU8DlSFpy03PzuzHdTTIBuonn5NGm5NJFO3p2sTXc+t0enPFghMUAcy8hGNvYFc9IzB7H378WblCRHLxN4+15t49VZPwwn6VRr6lASFFgSKL0kSkkDY8UUI5zaA1I8fsO8N9azC3IaoSQb6Gvx5FtQHtdWw792/nc6nE50vDXIrxXKM/4ICr5mLy1/drUIN8qZDDG6Kb4po0y9mk2fOHFaE7bHR7tIvLVY0vNSGgR/BmpAlSpaIGXo6MIFDKMiZmqiPFTEGJYYbOPusb9SY2U0HNL5dEnEqC7dN/C9W6oreSS3nmQz59pyYlaFt+YVt9z/w1ki9Htm7ujxifzSJr12N7OtMmTDJiXhK8wf8EPLA6y2wLdWfSkitMb848D0ELqwk7/q2z1xo/r1PG+/d/Khvoz6PtPwtA++KOsvjTTvv81qYs61Z2W9hTCuKNFluQzyaZuzPRrskzmzGCDc8h85o7NWNreMZiEAIHF9xtIQefDsTseirekLV0dDg/1Sx2A3QtMaCNd0cn0ggXmpWbjUDykb5AIloiMTqU6QvURiNysAokrJ3fc0UscI2DGXDTS2PQoC1AgHRCZyDEG4lD5x5JDI4YgxebhmmzwJagGhY2Z8+qB0C4CWHw== delphus@insight

# Pre-install RAID setup
bootcmd:
  - modprobe raid1
  - modprobe md_mod

packages:
  - mdadm
  - nvme-cli
  - smartmontools
  - parted
  - lvm2
  - open-iscsi
  - nfs-common
  - nfs-kernel-server

write_files:
  - path: /tmp/setup-optimized-raid.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/bin/bash
      # Optimized RAID1 Setup with Storage Partitioning

      set -e

      echo "Setting up optimized RAID1 arrays with partitioned storage..."

      # Stop any existing arrays
      mdadm --stop --scan 2>/dev/null || true

      # Zero superblocks
      mdadm --zero-superblock /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 2>/dev/null || true

      # Create RAID1 array for OS + Data (960GB drives)
      echo "Creating RAID1 array: /dev/md0 (960GB - OS + Data)"
      mdadm --create /dev/md0 \
        --level=1 \
        --raid-devices=2 \
        --metadata=1.2 \
        --assume-clean \
        /dev/nvme0n1 /dev/nvme1n1

      # Create RAID1 array for storage (1.92TB drives)
      echo "Creating RAID1 array: /dev/md1 (1.92TB - Pure Storage)"
      mdadm --create /dev/md1 \
        --level=1 \
        --raid-devices=2 \
        --metadata=1.2 \
        --assume-clean \
        /dev/nvme2n1 /dev/nvme3n1

      # Wait for arrays to be ready
      sleep 10

      # Partition the first RAID1 array (md0)
      echo "Partitioning /dev/md0 for OS and data..."
      parted -s /dev/md0 mklabel gpt
      parted -s /dev/md0 mkpart primary ext4 0% 200GB      # OS partition
      parted -s /dev/md0 mkpart primary ext4 200GB 100%   # Data partition

      # Wait for partition creation
      sleep 5

      # Format partitions
      echo "Formatting partitions..."
      mkfs.ext4 -F -L "harvester-root" /dev/md0p1    # OS
      mkfs.ext4 -F -L "harvester-data" /dev/md0p2    # Additional storage

      # Format storage RAID
      mkfs.ext4 -F -L "harvester-storage" /dev/md1   # Main storage

      # Create mount points
      mkdir -p /mnt/data
      mkdir -p /mnt/storage

      # Mount additional storage partitions
      mount /dev/md0p2 /mnt/data
      mount /dev/md1 /mnt/storage

      # Add to fstab for persistent mounting
      echo "UUID=$(blkid -s UUID -o value /dev/md0p2) /opt/harvester/data ext4 defaults,noatime 0 2" >> /etc/fstab
      echo "UUID=$(blkid -s UUID -o value /dev/md1) /opt/harvester/storage ext4 defaults,noatime 0 2" >> /etc/fstab

      # Save RAID configuration
      mdadm --detail --scan >> /etc/mdadm/mdadm.conf

      # Display configuration
      echo "=== RAID Configuration ==="
      cat /proc/mdstat
      echo
      echo "=== Partition Layout ==="
      lsblk /dev/md0 /dev/md1
      echo
      echo "=== Storage Allocation ==="
      echo "OS Partition (md0p1):   200GB"
      echo "Data Partition (md0p2): ~760GB"
      echo "Storage Array (md1):    ~1.92TB"
      echo "Total Storage Pool:     ~2.68TB"

      echo "Optimized RAID setup completed successfully"

  - path: /tmp/storage-pool-setup.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/bin/bash
      # Setup Longhorn storage with multiple disks

      echo "Configuring Longhorn storage pools..."

      # Wait for Harvester to be ready
      while ! kubectl get nodes >/dev/null 2>&1; do
        echo "Waiting for Kubernetes to be ready..."
        sleep 10
      done

      # Create storage directories
      mkdir -p /opt/harvester/data/longhorn
      mkdir -p /opt/harvester/storage/longhorn

      # Set proper permissions
      chown -R root:root /opt/harvester/data/longhorn
      chown -R root:root /opt/harvester/storage/longhorn
      chmod 755 /opt/harvester/data/longhorn
      chmod 755 /opt/harvester/storage/longhorn

      # Apply Longhorn disk configuration
      kubectl apply -f /tmp/longhorn-multi-disk-config.yaml

      echo "Storage pool configuration completed"

  - path: /tmp/longhorn-multi-disk-config.yaml
    permissions: "0644"
    owner: root:root
    content: |
      # Longhorn configuration for multiple storage locations
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: longhorn-storageclass
        namespace: longhorn-system
      data:
        storageclass.yaml: |
          # Fast storage class (760GB from first RAID1)
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: longhorn-fast
          provisioner: driver.longhorn.io
          allowVolumeExpansion: true
          parameters:
            numberOfReplicas: "1"
            diskSelector: "fast"
            nodeSelector: ""
            dataLocality: "strict-local"
            fsType: "ext4"
          ---
          # Large storage class (1.92TB RAID1)
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: longhorn-large
            annotations:
              storageclass.kubernetes.io/is-default-class: "true"
          provisioner: driver.longhorn.io
          allowVolumeExpansion: true
          parameters:
            numberOfReplicas: "1"
            diskSelector: "large"
            nodeSelector: ""
            dataLocality: "strict-local"
            fsType: "ext4"
          ---
          # Combined storage class (uses both pools)
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: longhorn-combined
          provisioner: driver.longhorn.io
          allowVolumeExpansion: true
          parameters:
            numberOfReplicas: "1"
            diskSelector: ""
            nodeSelector: ""
            dataLocality: "best-effort"
            fsType: "ext4"

  - path: /tmp/disk-labels-setup.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/bin/bash
      # Setup disk labels for Longhorn

      echo "Setting up disk labels for Longhorn storage..."

      # Wait for node to be ready
      while ! kubectl get nodes >/dev/null 2>&1; do
        sleep 5
      done

      NODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')

      # Label the node and configure disks
      kubectl patch node "$NODE_NAME" -p '{
        "metadata": {
          "annotations": {
            "node.longhorn.io/default-disks-config": "[{\"path\":\"/opt/harvester/data/longhorn\",\"allowScheduling\":true,\"tags\":[\"fast\",\"ssd\",\"raid1\"]},{\"path\":\"/opt/harvester/storage/longhorn\",\"allowScheduling\":true,\"tags\":[\"large\",\"ssd\",\"raid1\"]}]"
          }
        }
      }'

      echo "Disk configuration applied to node $NODE_NAME"
  - path: /etc/sysctl.d/99-storage-optimization.conf
    permissions: "0644"
    owner: root:root
    content: |
      # Storage optimization for multiple RAID arrays
      vm.dirty_ratio=15
      vm.dirty_background_ratio=5
      vm.dirty_writeback_centisecs=100
      vm.dirty_expire_centisecs=200

# Harvester configuration for optimized RAID
scheme_version: 1
server_url: ""
token: "harvester-cluster-token"
vip: 148.113.208.186
vip_mode: static

networks:
    harvester-mgmt:
      interfaces:
        - name: harvester-mgmt
          hwaddr: ""
      method: dhcp

  # Disk configuration - now pointing to partitioned storage
  disks:
    - device: /opt/harvester/data/longhorn     # 760GB from first RAID1
      allow_scheduling: true
      eviction_requested: false
      force_formatted: false
      tags:
        - fast
        - ssd
        - raid1
    - device: /opt/harvester/storage/longhorn  # 1.92TB from second RAID1
      allow_scheduling: true
      eviction_requested: false
      force_formatted: false
      tags:
        - large
        - ssd
        - raid1

  install:
    mode: create
    management_interface:
      interfaces:
        - name: eth0
          hwaddr: ""
      method: dhcp
    device: /dev/md0p1    # Install OS on 200GB partition
    data_disk: ""         # We'll handle data disks manually
    tty: ttyS0,115200n8

# Run setup commands
runcmd:
  - /tmp/setup-optimized-raid.sh
  - sleep 10
  - mkdir -p /opt/harvester/data
  - mkdir -p /opt/harvester/storage
  - mount /dev/md0p2 /opt/harvester/data
  - mount /dev/md1 /opt/harvester/storage
  - /tmp/storage-pool-setup.sh
  - /tmp/disk-labels-setup.sh
  - sysctl -p /etc/sysctl.d/99-storage-optimization.conf
  - systemctl enable open-iscsi
  - systemctl start open-iscsi
  - systemctl enable nfs-kernel-server
  - systemctl enable harvester
  - systemctl start harvester

power_state:
  mode: reboot
  delay: "+2"
  message: "Rebooting to complete optimized Harvester installation"
  condition: true
