# Harvester Configuration with Software RAID1 Setup
# RAID1: 2×960GB NVMe for OS/System
# RAID1: 2×1.92TB NVMe for Storage

#cloud-config

hostname: harvester-epyc-raid-01
fqdn: harvester-epyc-raid-01.local

users:
  - name: rancher
    groups:
      - sudo
      - docker
    sudo: ALL=(ALL) NOPASSWD:ALL
    shell: /bin/bash
    ssh_authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2E... # Replace with your SSH public key

# Pre-install RAID setup
bootcmd:
  - modprobe raid1
  - modprobe md_mod

packages:
  - mdadm
  - nvme-cli
  - smartmontools
  - fio
  - htop
  - iotop

write_files:
  - path: /tmp/setup-raid.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/bin/bash
      # Software RAID1 Setup Script for Harvester

      set -e

      echo "Setting up Software RAID1 arrays..."

      # Stop any existing arrays
      mdadm --stop --scan 2>/dev/null || true

      # Zero superblocks if they exist
      mdadm --zero-superblock /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 2>/dev/null || true

      # Create RAID1 array for OS (960GB drives)
      echo "Creating RAID1 array for OS: /dev/md0 (960GB)"
      mdadm --create /dev/md0 \
        --level=1 \
        --raid-devices=2 \
        --metadata=1.2 \
        --assume-clean \
        /dev/nvme0n1 /dev/nvme1n1

      # Create RAID1 array for storage (1.92TB drives)
      echo "Creating RAID1 array for storage: /dev/md1 (1.92TB)"
      mdadm --create /dev/md1 \
        --level=1 \
        --raid-devices=2 \
        --metadata=1.2 \
        --assume-clean \
        /dev/nvme2n1 /dev/nvme3n1

      # Wait for arrays to be ready
      echo "Waiting for RAID arrays to sync..."
      sleep 5

      # Check array status
      cat /proc/mdstat
      mdadm --detail /dev/md0
      mdadm --detail /dev/md1

      # Save RAID configuration
      mdadm --detail --scan >> /etc/mdadm/mdadm.conf

      echo "RAID setup completed successfully"

  - path: /tmp/raid-monitoring.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/bin/bash
      # RAID monitoring script

      echo "=== RAID Status Check ==="
      echo "Date: $(date)"
      echo

      echo "RAID Arrays:"
      cat /proc/mdstat
      echo

      echo "Detailed Status:"
      for array in /dev/md*; do
        if [ -b "$array" ]; then
          echo "--- $array ---"
          mdadm --detail "$array"
          echo
        fi
      done

      echo "Disk Health:"
      for disk in nvme0n1 nvme1n1 nvme2n1 nvme3n1; do
        if [ -b "/dev/$disk" ]; then
          echo "--- /dev/$disk ---"
          smartctl -H "/dev/$disk" | grep -E "(SMART|health)"
          nvme smart-log "/dev/$disk" | grep -E "(temperature|available_spare|percentage_used)"
          echo
        fi
      done

  - path: /etc/systemd/system/raid-monitoring.service
    permissions: "0644"
    owner: root:root
    content: |
      [Unit]
      Description=RAID Monitoring Service
      After=multi-user.target

      [Service]
      Type=oneshot
      ExecStart=/tmp/raid-monitoring.sh

  - path: /etc/systemd/system/raid-monitoring.timer
    permissions: "0644"
    owner: root:root
    content: |
      [Unit]
      Description=Run RAID monitoring every hour
      Requires=raid-monitoring.service

      [Timer]
      OnCalendar=hourly
      Persistent=true

      [Install]
      WantedBy=timers.target

  - path: /etc/mdadm/mdadm.conf
    permissions: "0644"
    owner: root:root
    content: |
      # mdadm configuration file
      DEVICE partitions
      CREATE owner=root group=disk mode=0660 auto=yes
      HOMEHOST <system>
      MAILADDR root

  - path: /tmp/raid-optimization.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/bin/bash
      # RAID and NVMe optimization

      echo "Optimizing RAID arrays and NVMe drives..."

      # Set optimal chunk size and stride for ext4 on RAID1
      # RAID1 doesn't use chunks, but we optimize the underlying NVMe

      # NVMe optimizations for RAID members
      for disk in nvme0n1 nvme1n1 nvme2n1 nvme3n1; do
        if [ -b "/dev/$disk" ]; then
          # Set scheduler to none for NVMe
          echo none > /sys/block/$disk/queue/scheduler

          # Optimize queue depth
          echo 1024 > /sys/block/$disk/queue/nr_requests

          # Set read-ahead
          echo 512 > /sys/block/$disk/queue/read_ahead_kb

          echo "Optimized /dev/$disk"
        fi
      done

      # RAID array optimizations
      for array in md0 md1; do
        if [ -b "/dev/$array" ]; then
          # Set read-ahead for RAID array
          echo 1024 > /sys/block/$array/queue/read_ahead_kb

          # Set optimal scheduler for RAID
          echo mq-deadline > /sys/block/$array/queue/scheduler

          echo "Optimized /dev/$array"
        fi
      done

      # Enable write-intent bitmap for better recovery
      mdadm --grow --bitmap=internal /dev/md0 2>/dev/null || true
      mdadm --grow --bitmap=internal /dev/md1 2>/dev/null || true

      echo "RAID optimization completed"

# Harvester configuration for RAID arrays
harvester_config:
  server_url: ""
  token: ""

  networks:
    harvester-mgmt:
      interfaces:
        - name: harvester-mgmt
          hwaddr: ""
      method: dhcp
      bond_options:
        mode: active-backup
        miimon: 100

  # RAID-based disk configuration
  disks:
    - device: /dev/md0  # 960GB RAID1 - OS and system
      allow_scheduling: false
      eviction_requested: false
      force_formatted: true
      tags:
        - system
        - raid1
        - fast
    - device: /dev/md1  # 1.92TB RAID1 - Storage
      allow_scheduling: true
      eviction_requested: false
      force_formatted: true
      tags:
        - storage
        - raid1
        - large

  install:
    mode: create
    management_interface:
      interfaces:
        - name: eth0
          hwaddr: ""
      method: dhcp
    device: /dev/md0      # Install OS on 960GB RAID1
    data_disk: /dev/md1   # Use 1.92TB RAID1 for data
    tty: ttyS0,115200n8

  # Storage optimization settings
  data_dir: /opt/harvester

# System optimizations
write_files:
  - path: /etc/sysctl.d/99-raid-optimization.conf
    permissions: "0644"
    owner: root:root
    content: |
      # RAID and NVMe optimization settings

      # Memory management for RAID
      vm.dirty_ratio=20
      vm.dirty_background_ratio=10
      vm.dirty_expire_centisecs=3000
      vm.dirty_writeback_centisecs=500

      # Network optimizations
      net.core.rmem_max=134217728
      net.core.wmem_max=134217728
      net.ipv4.tcp_rmem=4096 131072 134217728
      net.ipv4.tcp_wmem=4096 65536 134217728

  - path: /tmp/longhorn-raid-config.yaml
    permissions: "0644"
    owner: root:root
    content: |
      # Longhorn storage class for RAID configuration
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: longhorn-raid1
        annotations:
          storageclass.kubernetes.io/is-default-class: "true"
      provisioner: driver.longhorn.io
      allowVolumeExpansion: true
      parameters:
        numberOfReplicas: "1"  # RAID1 already provides redundancy
        staleReplicaTimeout: "2880"
        diskSelector: "raid1"
        nodeSelector: ""
        dataLocality: "strict-local"
        fsType: "ext4"
        mkfsParams: "-F -E lazy_itable_init=0,lazy_journal_init=0 -b 4096"
      ---
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: longhorn-raid1-replicated
      provisioner: driver.longhorn.io
      allowVolumeExpansion: true
      parameters:
        numberOfReplicas: "2"  # Double redundancy (RAID1 + Longhorn)
        staleReplicaTimeout: "2880"
        diskSelector: "raid1"
        nodeSelector: ""
        dataLocality: "best-effort"
        fsType: "ext4"
        mkfsParams: "-F -E lazy_itable_init=0,lazy_journal_init=0 -b 4096"

# Run commands in order
runcmd:
  - /tmp/setup-raid.sh
  - sleep 10
  - /tmp/raid-optimization.sh
  - systemctl enable raid-monitoring.timer
  - systemctl start raid-monitoring.timer
  - sysctl -p /etc/sysctl.d/99-raid-optimization.conf
  - update-initramfs -u
  - systemctl enable harvester
  - systemctl start harvester

power_state:
  mode: reboot
  delay: "+2"
  message: "Rebooting to complete RAID1 Harvester installation"
  condition: true