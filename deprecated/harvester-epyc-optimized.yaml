# Harvester Configuration Optimized for AMD EPYC 8224P
# 24 cores / 48 threads - 192GB ECC RAM - 4x NVMe drives

#cloud-config

hostname: harvester-epyc-01
fqdn: harvester-epyc-01.local

users:
  - name: rancher
    groups:
      - sudo
      - docker
    sudo: ALL=(ALL) NOPASSWD:ALL
    shell: /bin/bash
    ssh_authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2E... # Replace with your SSH public key

harvester_config:
  server_url: ""
  token: ""

  # Optimized network configuration for high-performance
  networks:
    harvester-mgmt:
      interfaces:
        - name: harvester-mgmt
          hwaddr: ""
      method: dhcp
      bond_options:
        mode: 802.3ad    # LACP for better performance
        miimon: 100
        lacp_rate: fast
        xmit_hash_policy: layer3+4

  # NVMe storage configuration
  disks:
    - device: /dev/nvme0n1  # 960GB - OS + System
      allow_scheduling: false
      eviction_requested: false
      force_formatted: true
      tags:
        - system
        - nvme-fast
    - device: /dev/nvme1n1  # 960GB - High-performance workloads
      allow_scheduling: true
      eviction_requested: false
      force_formatted: true
      tags:
        - compute
        - nvme-fast
        - high-iops
    - device: /dev/nvme2n1  # 1.92TB - Primary storage
      allow_scheduling: true
      eviction_requested: false
      force_formatted: true
      tags:
        - storage
        - nvme-large
        - primary
    - device: /dev/nvme3n1  # 1.92TB - Secondary storage
      allow_scheduling: true
      eviction_requested: false
      force_formatted: true
      tags:
        - storage
        - nvme-large
        - secondary

  install:
    mode: create
    management_interface:
      interfaces:
        - name: eth0
          hwaddr: ""
      method: dhcp
    device: /dev/nvme0n1
    data_disk: /dev/nvme2n1
    tty: ttyS0,115200n8

packages:
  - nvme-cli
  - fio
  - iozone3
  - htop
  - iotop
  - tcpdump
  - ethtool
  - numactl

timezone: UTC

write_files:
  # EPYC-specific optimizations
  - path: /etc/sysctl.d/99-epyc-optimization.conf
    permissions: "0644"
    owner: root:root
    content: |
      # AMD EPYC optimization settings

      # Memory management for large RAM systems
      vm.swappiness=1
      vm.dirty_ratio=15
      vm.dirty_background_ratio=5
      vm.vfs_cache_pressure=50
      vm.min_free_kbytes=2097152

      # Network optimizations for high-speed networking
      net.core.rmem_max=134217728
      net.core.wmem_max=134217728
      net.core.rmem_default=262144
      net.core.wmem_default=262144
      net.core.netdev_max_backlog=5000
      net.ipv4.tcp_rmem=4096 131072 134217728
      net.ipv4.tcp_wmem=4096 65536 134217728
      net.ipv4.tcp_congestion_control=bbr

      # NVMe optimizations
      block.nr_requests=1024

      # NUMA optimizations for EPYC
      kernel.numa_balancing=0

  - path: /tmp/epyc-performance-tuning.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/bin/bash
      # AMD EPYC 8224P Performance Optimization Script

      echo "Applying EPYC-specific optimizations..."

      # CPU frequency scaling
      echo performance > /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

      # Disable CPU idle states for consistent performance
      for i in /sys/devices/system/cpu/cpu*/cpuidle/state*/disable; do
        [ -f "$i" ] && echo 1 > "$i"
      done

      # NVMe optimizations
      for disk in nvme0n1 nvme1n1 nvme2n1 nvme3n1; do
        if [ -b "/dev/$disk" ]; then
          # Set optimal scheduler for NVMe
          echo none > /sys/block/$disk/queue/scheduler

          # Optimize queue depth
          echo 1024 > /sys/block/$disk/queue/nr_requests

          # Set optimal read-ahead
          echo 512 > /sys/block/$disk/queue/read_ahead_kb

          # Enable write cache
          echo write back > /sys/block/$disk/queue/write_cache 2>/dev/null || true

          echo "Optimized $disk"
        fi
      done

      # NUMA memory policy for containers
      echo "Setting NUMA policies..."
      numactl --hardware

      # Hugepages configuration (10GB for VM workloads)
      echo 5120 > /proc/sys/vm/nr_hugepages

      # IRQ affinity optimization
      echo "Optimizing IRQ affinity..."
      for irq in $(cat /proc/interrupts | grep nvme | awk -F: '{print $1}'); do
        # Distribute NVMe IRQs across cores
        core=$((irq % 24))
        echo $((2**core)) > /proc/irq/$irq/smp_affinity 2>/dev/null || true
      done

      echo "EPYC optimization completed"

  - path: /tmp/longhorn-epyc-config.yaml
    permissions: "0644"
    owner: root:root
    content: |
      # Longhorn storage classes optimized for EPYC + NVMe
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: longhorn-nvme-fast
        annotations:
          storageclass.kubernetes.io/is-default-class: "false"
      provisioner: driver.longhorn.io
      allowVolumeExpansion: true
      parameters:
        numberOfReplicas: "2"
        staleReplicaTimeout: "2880"
        diskSelector: "nvme-fast"
        nodeSelector: ""
        dataLocality: "best-effort"
        fsType: "ext4"
        mkfsParams: "-F -E lazy_itable_init=0,lazy_journal_init=0"
      ---
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: longhorn-nvme-large
        annotations:
          storageclass.kubernetes.io/is-default-class: "true"
      provisioner: driver.longhorn.io
      allowVolumeExpansion: true
      parameters:
        numberOfReplicas: "2"
        staleReplicaTimeout: "2880"
        diskSelector: "nvme-large"
        nodeSelector: ""
        dataLocality: "best-effort"
        fsType: "ext4"
        mkfsParams: "-F -E lazy_itable_init=0,lazy_journal_init=0"
      ---
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: longhorn-high-iops
      provisioner: driver.longhorn.io
      allowVolumeExpansion: true
      parameters:
        numberOfReplicas: "1"
        staleReplicaTimeout: "2880"
        diskSelector: "high-iops"
        nodeSelector: ""
        dataLocality: "strict-local"
        fsType: "ext4"
        mkfsParams: "-F -E lazy_itable_init=0,lazy_journal_init=0"

  - path: /tmp/resource-allocation.yaml
    permissions: "0644"
    owner: root:root
    content: |
      # Resource allocation for EPYC system
      apiVersion: v1
      kind: LimitRange
      metadata:
        name: epyc-resource-limits
        namespace: default
      spec:
        limits:
        - default:
            cpu: "4"      # Default 4 cores per container
            memory: "8Gi" # Default 8GB per container
          defaultRequest:
            cpu: "500m"   # Default 0.5 cores request
            memory: "1Gi" # Default 1GB request
          type: Container
        - max:
            cpu: "20"     # Max 20 cores per container (leave 4 for system)
            memory: "160Gi" # Max 160GB per container (leave 32GB for system)
          type: Container
      ---
      apiVersion: v1
      kind: ResourceQuota
      metadata:
        name: epyc-namespace-quota
        namespace: default
      spec:
        hard:
          requests.cpu: "20"
          requests.memory: "80Gi"
          limits.cpu: "40"
          limits.memory: "160Gi"
          persistentvolumeclaims: "50"

  - path: /etc/systemd/system/epyc-optimization.service
    permissions: "0644"
    owner: root:root
    content: |
      [Unit]
      Description=AMD EPYC Performance Optimization
      After=multi-user.target

      [Service]
      Type=oneshot
      ExecStart=/tmp/epyc-performance-tuning.sh
      RemainAfterExit=yes

      [Install]
      WantedBy=multi-user.target

runcmd:
  - sysctl -p /etc/sysctl.d/99-epyc-optimization.conf
  - systemctl enable epyc-optimization.service
  - systemctl start epyc-optimization.service
  - /tmp/epyc-performance-tuning.sh
  - systemctl enable harvester
  - systemctl start harvester

power_state:
  mode: reboot
  delay: "+2"
  message: "Rebooting to complete EPYC-optimized Harvester installation"
  condition: true